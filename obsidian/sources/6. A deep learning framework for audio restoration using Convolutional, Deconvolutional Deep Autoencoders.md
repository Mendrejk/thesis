[[A deep learning framework for audio restoration using Convolutional, Deconvolutional Deep Autoencoders.pdf]]

## A B S T R A C T
People communicate daily with their mobile phones and in some cases, the quality of the communication may be vital. Thus, there is a clear interest in improving the quality of communication in cases of low signal or interferences. This paper shows **how deep learning techniques are used to restore audio files that simulate situations of background noise and loss of signal**. Its main distinguishing feature is the **direct use of the waveform instead of a spectrogram representation** which lets the model be adapted to **real-time communications or broadcasting**. The results show that our proposal improves performance compared to Wave-U-Net. After restoring the audio, the **difference between the original and the restored audio is, on average, less than 2%.** In addition, a subjective test was carried out with 113 **people who detected a significant improvement** in the restored audio compared to the damaged one.


## 5. Conclusions and future work 
The goal of this research is the **restoration of audio files damaged by noise or loss of information** through the application of deep learning techniques. It was decided to use **speech audio files rather than music** or other content because these audio files are like the use cases of phone calls proposed at the beginning. In speech files, it is also easier to evaluate objective (word count) and subjective (perceived understand- ability) results. Files were found and prepared for the research. To best suit the two use cases, a thorough study was made on how to create damaged audio files which resemble real-world situations. In the case of noise, **it was decided to work with (Gaussian White Noise sinusoidal wave) GWN**, since it is very similar to many random processes that occur in the real world. In the case of loss of information, the **audio files were damaged by eliminating part of their samples as randomly** as possible. Once all the audio files were damaged and the final dataset was built by normalizing, a deep learning model was developed. Its architecture consisted of an hourglass architecture using **convolutional and deconvolutional 1-dimension layers along with pooling and skip connection**. After training two twin models with two different datasets, one for each use case, an accuracy of 98% was obtained. Audio files with loss of information were restored in the same percentage. These results were confirmed both objectively and subjectively. The model can be used in real-time streaming, provided that the initial delay is acceptable to the broadcaster (either radio or television). As it stands now, the clean audio exits the model 2 s after streaming starts. This time is the duration of the input data needed by the neural model. The pre, post, and processing (split, clean, and paste) times are negligible. However, the 2-second delay may not be acceptable for a live stream, and we are working on reducing this initial lag using several strategies to be published in a future paper. Future research could continue focusing on various tracks. For example, working with higher quality audio files and/or stereo. The main limitation of the research was the size of the data. It was not possible to make use of higher quality due to computational and time limitations. If these two factors could be improved, this study could be continued exactly as conducted thus far, but with more data, higher sampling frequency, and a greater number of channels. Working with music. For reasons like those indicated above, this project did not work with music files because of the size and quality of the data. A possible continuation or reorientation of this study could be to create a new training dataset but with music files rather than speech or voice files. Music files can be damaged, or the model trained just as in this research. Other noise and damage. As explained, the noise used to damage the audio files was Gaussian White Noise. A very interesting way to continue this research would be to study different scenarios or real-world cases where another type of noise is produced and learn how to reproduce it and so damage the audio files. Thus, after training the model with this new dataset, it will most likely work in new scenarios and with new use cases. 

### Proposal of new use cases. 
For example, it would be interesting to make **the same study by restoring audio that comprises both cases presented in this work: background noise and loss of information**. On the other hand, what has been achieved in this research **could be continued for the creation of real applications** such as streaming audio reconstruction. It has been shown that the performance of the network allows real-time reconstructions, and this could be a possible application. It would therefore be possible to improve the quality of telephone calls in emergency situations. Additionally, if it is possible to recreate the same noise or damage, it may be possible to train the model with this new case. This would allow the development of various applications such as the reconstruction of old audio files, accident recordings, etc.

# TLDR

Autorzy podchodzą do problemu przetwarzania dźwięku, głównie konwersacji, "na żywo". W tym celu korzystają z przetwarzania samego przebiegu fali dźwiękowej.
Autorzy tworzą sieć typu U-Net na bazie warts konwolucji, dekonwolucji 1 wymiarowej, poolingu i "skip connection".
Dalsze badania mogą być prowadzone w kierunku zastosowania tej sieci w faktycznych aplikacjach oraz powtórzeniu badania w celu eliminacji szumów oraz odzyskiwania utraconych informacji.